{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie reviews sentiment classifier\n",
    "\n",
    "**CONTEXT**: The objective of this project is to build a text classification model that analyses the customer's sentiments based on their reviews in the IMDB database. The model uses a complex deep learning model to build an embedding layer followed by\n",
    "a classification algorithm to analyse the sentiment of the customers.\n",
    "\n",
    "**DATA DESCRIPTION**: The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocabulary size of 10,000. As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "**PROJECT OBJECTIVE**: Build a sequential NLP classifier which can use input text parameters to determine the customer sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries\n",
    "\n",
    "We import the classes and functions required for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We need to load the IMDB dataset. We are constraining the dataset to the top 10,000 words. We also split the dataset into train (50%) and test (50%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 10000 ## Taking 10000 most frequent words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[11].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode the feature value to get original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> when i rented this movie i had very low expectations but when i saw it i realized that the movie was less a lot less than what i expected the actors were bad the doctor's wife was one of the worst the story was so stupid it could work for a disney movie except for the murders but this one is not a comedy it is a laughable masterpiece of stupidity the title is well chosen except for one thing they could add stupid movie after dead husbands i give it 0 and a half out of 5\n"
     ]
    }
   ],
   "source": [
    "def get_sentences(feature):\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "    word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "    S = ' '.join(id_to_word[id] for id in feature)\n",
    "    print(S)\n",
    "\n",
    "get_sentences(X_train[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate and pad input sequences\n",
    "\n",
    "We need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train_em = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test_em = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_em[11].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the model\n",
    "\n",
    "The first layer is the **Embedded layer** that uses 32 length vectors to represent each word. The next layer is the **LSTM layer** with 100 memory units. Finally, because this is a classification problem we use a **Dense output layer** with a single neuron and a **sigmoid activation function** to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n",
    "\n",
    "Because it is a binary classification problem, **binary_crossentropy** loss is used as the loss function. The efficient **ADAM optimization algorithm** is used. The model is fit for only 5 epochs because it quickly overfits the problem. A large batch size of 64 reviews is used to space out weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 617s 2s/step - loss: 0.5630 - accuracy: 0.6814\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 655s 2s/step - loss: 0.2645 - accuracy: 0.8999\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 630s 2s/step - loss: 0.2229 - accuracy: 0.9189\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 639s 2s/step - loss: 0.1877 - accuracy: 0.9315\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 633s 2s/step - loss: 0.1654 - accuracy: 0.9408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1da5a17a148>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_em, y_train, epochs=5, batch_size=64) ## Increase the epochs as per your hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.88%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test_em, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on any one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test_em, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(val):\n",
    "    if val==1:\n",
    "        return 'Positive Review'\n",
    "    return 'Negative Review'\n",
    "\n",
    "def get_predictions(n):\n",
    "    review = get_sentences(X_test[n])\n",
    "    print('-----------------------------------')\n",
    "    print('Predicted sentiment -', sentiment(y_pred[n]))\n",
    "    print('-----------------------------------')\n",
    "    print('Actual sentiment    -', sentiment(y_test[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> here's a couple of <UNK> out of an <UNK> i wrote for university about br br the book of revelation is an erotic thriller about sex power and a talented <UNK> struggle to regain his sense of self after being unfortunately raped by three <UNK> women the three women that <UNK> him all have distinctive marks on the bodies one has a giant birth mark on her <UNK> another has a <UNK> <UNK> on her lower stomach and the ring leader has a small circle on her breast so he lives his new life in search of these <UNK> and to find them on these intimate places he does what any sane man does when he needs to see as many naked women as possible to solve a mystery he has sex with them an hour and ten minutes into the film and you feel like he has almost had a piece of every woman in melbourne br br the film is a giant <UNK> of pretentious celluloid it is like <UNK> from every frame at only one point towards the films final climax does give a scene the same energy and strength as her debut feature head on had in <UNK> as like many films <UNK> by the government bodies the film takes it self way to seriously the script and its execution appear to be <UNK> rather then gifts and unfortunately for the talented <UNK> their brilliant performances particularly tom long as the <UNK> protagonist are stuck within the confines of a pompous wan k fest\n",
      "-----------------------------------\n",
      "Predicted sentiment - Negative Review\n",
      "-----------------------------------\n",
      "Actual sentiment    - Negative Review\n"
     ]
    }
   ],
   "source": [
    "get_predictions(144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> i actually found out about rising via the imdb website i have a particular interest in <UNK> brazilian culture and films rising is one of those gems that gives a new meaning to human transformation beautifully documented and filmed by jeff <UNK> and matt its the story anderson <UNK> a former <UNK> de <UNK> drug <UNK> who after the deaths of family members and friends becomes a christ like malcolm x and <UNK> all rolled into one <UNK> formed a <UNK> cultural movement that uses <UNK> brazilian <UNK> <UNK> brazilian martial arts <UNK> and other to <UNK> the hopeless and most times angry youth into vibrant <UNK> caring community loving individuals br br a few years ago i remember going to a screening of city of god de <UNK> and walked out of the theatre completely <UNK> the images were grim yet stunning and you couldn't take your eyes off the screen i remember how hopeless some situations were in the <UNK> and how <UNK> the society was due to the <UNK> neglect how drug <UNK> was a way of life how indifferent the citizens of the <UNK> were because death was an every day <UNK> like city of god anderson <UNK> talks about how the people of the <UNK> were also <UNK> he talks about the police corruption and how the <UNK> were so by drugs and gangs that you couldn't visit family members in other <UNK> you had to meet in a neutral location unlike city of god anderson <UNK> movement provides <UNK> to the anger the <UNK> br br there was one part in the documentary where anderson in the spirit of a preacher approached some youth and asked them to join these jaded youth were so <UNK> by everyday survival and violence their role models were drug dealers and this is what they <UNK> to be anderson told then that drug dealers don't live very long there was <UNK> of course but five months later he was able to get some of the youth to join br br the visuals in rising are beyond amazing its clear to me that jeff <UNK> and matt are not only great story <UNK> but visual artist as well this is a must see documentary there are some really magical and <UNK> moments in this documentary i don't want to spoil them for you i want you see it for yourself please tell your friends <UNK> youth <UNK> family members about this wonderful film it will make you care about the world and our children br br i would give it eleven stars\n",
      "-----------------------------------\n",
      "Predicted sentiment - Positive Review\n",
      "-----------------------------------\n",
      "Actual sentiment    - Positive Review\n"
     ]
    }
   ],
   "source": [
    "get_predictions(155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclution:\n",
    "\n",
    "Our model perform quite well with the predictions. The accuracy achieved with test data is 86.88% and the couple of predictions that were tried showed positive results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
